---
title: "ML Exercises"
author: "Jonthan Jan, Mu An Shen, Yen Wen Ting, Yi-Ting Huang"
date: "8/15/2021"
output: 
  md_document:
    variant: markdown_github
---

```{r, echo=False}
library(knitr)
opts_chunk$set(dev="png")
```

# Visual story telling part 1: green buildings

To begin with the analysis, we first processed and cleaned the data by making categorical variables to factors, dropped missing data, and combined certain variables into one to make analysis convenient. 

Also, I created a new variable, 'total rent', by taking the product of rent, leasing_rate, and size. 
In order to validate the stats guru's claim, we separated the data into green and non-green buildings, and we calculated the mean Rent and mean total rent for both subsets. 

```{r, echo=F,message=FALSE, warning=FALSE}
library(readr)
data = read_csv(url("https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv"))
data = data.frame(data)

#data cleaning
data$cluster = as.factor(data$cluster)
data$renovated = as.factor(data$renovated)
data$class_a = as.factor(data$class_a)
data$class_b = as.factor(data$class_b)
data$LEED = as.factor(data$LEED)
data$Energystar = as.factor(data$Energystar)
data$green_rating = as.factor(data$green_rating)
data$net = as.factor(data$net)
data$amenities = as.factor(data$amenities)
data = data[complete.cases(data),]
data = subset(data,select = -c(LEED,Energystar,cd_total_07,hd_total07))  #remove columns because they are included in another variable

#put all classes into one variable
for (i in 1:nrow(data)){
  if(data[i,'class_a']==1){
    data[i,'class'] = 3
    }else if(data[i,'class_b']==1){
    data[i,'class'] = 2
    }else{
    data[i,'class'] = 1
  }
}
data['class']=as.factor(data$class)
data = subset(data,select = -c(class_a,class_b))  #remove class_a and class_b columns as we already consolidated them into a new 'class' column

#create total_rent column, #take into account of the size and leasing rate of the building
data['total_rent'] = data$Rent * data$leasing_rate*0.01 *data$size

#separate green buildings 
green = data[data['green_rating']==1,]
normal = data[data['green_rating']!=1,]

cat('mean rent for green buildings = ', mean(green$Rent))
cat('\nmean rent for non-green buildings = ', mean(normal$Rent))
cat('\ndifference between mean rent of green buildings and non-green builings =',
    mean(green$Rent)-mean(normal$Rent))
cat('\nmean total rent for green buildings = ', mean(green$total_rent))
cat('\nmean total rent for non-green buildings = ', mean(normal$total_rent))
cat('difference between mean total rent of green buildings and non-green builings =',
    mean(green$total_rent)-mean(normal$total_rent))
```
```{r, echo=F,message=FALSE, warning=FALSE}
boxplot(normal$Rent, green$Rent, 
        ylab = 'rent', 
        names=c('non-green buildings', 'green buildings'))
```

Green building's Rent is higher than normal buildings in average and in total.

Next, we proceed to verify if green rating is what causes the increase in rent. First, we fit a linear model using most variables to identify significant predictors for green and normal buildings.

```{r, echo=F,message=FALSE, warning=FALSE}
summary(lm(Rent ~ size + empl_gr + leasing_rate + stories + age + renovated + net + 
amenities + total_dd_07 + Precipitation + Gas_Costs + Electricity_Costs + class, data = green))
``` 

empl_gr, leasing_rate, age,total_dd, precipitation, Gas_costs, Electricity_cost are significant predictors for green building rents. 

To visualize the individual effects of these significant predictors on Rent of green buildings and their correlations, we created a correlation plot, scatter plots, and box plots below:

```{r,echo=F,message=FALSE, warning=FALSE, out.width="33%"}
cormat <- round(cor(subset(normal,select = -c(cluster,renovated,green_rating,net,amenities,class, CS_PropertyID,cluster_rent,total_rent))),2)

library(reshape2)
library(tidyverse)
melted_cormat <- melt(cormat)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + geom_tile()+ theme(axis.text.x = element_text(angle = 45))
```
```{r,echo=F, message=FALSE, warning=FALSE,out.width="33%"}
library(gridExtra)
# rent ~ empl_gr
cor1 <- ggplot(green[which(green$empl_gr<10&green$empl_gr>-5),], aes(empl_gr,Rent))+geom_point(alpha = 0.3)+geom_smooth(method = 'lm')
# rent ~ leasing_rate
cor2 <- ggplot(green, aes(leasing_rate,Rent))+geom_point(alpha = 0.3)+geom_smooth(method = 'lm')
# rent ~ age
cor3 <- ggplot(green, aes(age,Rent))+geom_point(alpha = 0.3)+geom_smooth(method = 'lm')
# rent ~ total_dd_07
cor4 <- ggplot(green, aes(total_dd_07,Rent))+geom_point(alpha = 0.3)+geom_smooth(method = 'lm')
# rent ~ precipitation
cor5 <- ggplot(green, aes(Precipitation,Rent))+geom_point(alpha = 0.3)+geom_smooth(method = 'lm')
# rent ~ gas_cost
cor6 <- ggplot(green[which(green$Gas_Costs<0.02),],aes(Gas_Costs,Rent))+geom_point(alpha = 0.3)+geom_smooth(method = 'lm')
# rent ~ electricity_cost
cor7 <- ggplot(green,aes(Electricity_Costs,Rent))+geom_point(alpha = 0.3)+geom_smooth(method = 'lm')
grid.arrange(cor1,cor2,cor3,cor4,cor5,cor6,cor7, nrow=3, ncol=3)
```
```{r,echo=F, message=FALSE, warning=FALSE, out.width="33%"}
# rent ~ class
ggplot(green[which(green$Rent<75),],aes(class,Rent))+geom_boxplot()
```
By neglecting the outliers, we discovered most of these variables were significant to the increase of green building's rent. Then, we compared some of the variables of green buildings to those of normal buildings:

Let's start with age comparison: 
```{r,echo=F,message=FALSE, warning=FALSE, out.width="33%"}
summary(green$age)-summary(normal$age)
boxplot(green$age,normal$age, ylab = 'age', names = c('Green buildings', 'Non-green buildings')) 
```
We discovered that green builidings in general were newer, which leads to higher rent.

```{r,echo=F,message=FALSE, warning=FALSE}
p_green <- ggplot(data=green, aes(x=class, fill=class)) + geom_bar(stat='count') + 
                    ggtitle('Green buildings')+ scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))
p_normal <- ggplot(data=normal, aes(x=class, fill=class)) + geom_bar(stat='count')+
  ggtitle('Non-green buildings')+ scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))
grid.arrange(p_green, p_normal, ncol=2)
```
```{r}
summary(green$class)-summary(normal$class)

round(table(green$class)/sum(table(green$class)),2)
round(table(normal$class)/sum(table(normal$class)),2)
```
Most of the green buildings are in class A(= class score: 3 in our definition). This fact leads us to believe that green buildings are generally with higher quality compared to non-green buildings.

In conclusion, age and class could be the reason why green buildings were higher in rent instead of being rated as green buildings. As a result, we will recommend the developer to make build the new 15-story mixed-use building based on class A requirements.

# Visual story telling part 2: flights at ABIA

```{r,echo=F,message=FALSE, warning=FALSE,results='hide'}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(wesanderson)  #color palettes
library(gridExtra)  #arrange ggplot
airport = read.csv(url('https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv'))
```

We first created a new 'Date' column by combining 'Year', 'Month', and 'Day of Month'. We also converted the time-related columns(DepTime, CRSDepTime, ArrTime, CRSArrTime) into time formats(HH:MM) for convenient analysis. 

```{r,echo=F,message=FALSE, warning=FALSE,results='hide'}
# Create a 'Date' column by combining 'Year', 'Month' and 'Day of Month'
airport['Date'] = paste0(airport$Year, "-", airport$Month, "-", airport$DayofMonth)
# Convert 'Date' into Date-time format
airport$Date = as.POSIXct(airport$Date)  

# Convert 'Time' related columns into HH:MM formats
airport$DepTime = sprintf("%04d", airport$DepTime)
airport$DepTime = format(strptime(airport$DepTime, format="%H%M"), format = "%H:%M")
airport$CRSDepTime = sprintf("%04d", airport$CRSDepTime)
airport$CRSDepTime = format(strptime(airport$CRSDepTime, format="%H%M"), format = "%H:%M")
airport$ArrTime = sprintf("%04d", airport$ArrTime)
airport$ArrTime = format(strptime(airport$ArrTime, format="%H%M"), format = "%H:%M")
airport$CRSArrTime = sprintf("%04d", airport$CRSArrTime)
airport$CRSArrTime = format(strptime(airport$CRSArrTime, format="%H%M"), format = "%H:%M")
# Drop missing values
airport = airport[complete.cases(airport[,1:22]),]
```

Let's start from seeing the flight frequency by different scales of time:

```{r,echo=F,message=FALSE, warning=FALSE} 
# Plot flight frequency with Month
ggplot(data = airport %>% count(Month), aes(Month,n))+
  geom_line() +scale_x_continuous(breaks=c(1,2,3,4,5,6,7,8,9,10,11,12))+ ggtitle("Flight frequency by Month")
```

```{r,echo=F,message=FALSE, warning=FALSE}
p1 <- ggplot(data=airport, aes(x=DayofMonth)) + geom_line(stat='count') + ggtitle("Flight frequency by Day of Month in 12 months")
p1 + facet_wrap(~Month)
```
```{r,echo=F,message=FALSE, warning=FALSE}
ggplot(data = airport %>% count(DayOfWeek),aes(DayOfWeek,n))+ geom_line()+ ggtitle("Yearly flight frequency by Day of Week")
```
We can see that Winter generally had much fewer flights than Summer. This is understandable as people usually have vacations in the Summer. Moreover, the pattern of flight frequency by day of month looked similar in each month. On average, there were 4 to 5 drops in each month. We could guess this was because there was usually fewer flights in a specific day of week. The third plot actually made us realize that generally there ware fewer flights on Saturday(the sixth day of week).

```{r,echo=F,message=FALSE, warning=FALSE}
ggplot(data=airport, aes(x=fct_infreq(UniqueCarrier), fill=UniqueCarrier))+ geom_bar()+ ggtitle("Num of flights at ABIA airport by Carrier")
```
We can see that ABIA Airport were mainly dominated by Southwest Airlines(WN), while American Airlines(AA) and Continental Airlines(CO) being the Top 2 and Top 3 airlines.

```{r,echo=F,message=FALSE, warning=FALSE}
#split data into two
to_austin = airport[airport['Dest']=="AUS",]
from_austin = airport[airport['Dest']!="AUS",]
toAustinDelay = to_austin %>% group_by(UniqueCarrier) %>% summarise(mean=mean(ArrDelay,na.rm = T),n=n())
fromAustinDelay = from_austin %>% group_by(UniqueCarrier) %>% summarise(mean=mean(DepDelay,na.rm = T),n=n())
```

Let's try to dig more information of the Top 3 airlines:

To find insights with arrival or departure respectively, we separated the original full data into two data sets: one with all the flights flew to Austin and another one with all the flights departured from Austin.

```{r,echo=F,message=FALSE, warning=FALSE}
ad_carrier <- ggplot(toAustinDelay, aes(x=UniqueCarrier, y=mean, group = 1))+ geom_line()+ 
  ggtitle("Arrival delay to Austin by Carrier")+ labs(y='mean arrival delay(min)')
dd_carrier <- ggplot(fromAustinDelay, aes(x=UniqueCarrier, y=mean, group = 1))+ geom_line()+ 
  ggtitle("Departure delay from Austin by Carrier")+ labs(y='mean departure delay(min)')
grid.arrange(ad_carrier, dd_carrier, ncol=2)
```
We can see that the the performance of average delay rate of the Top 3 Airlines was medium among all airlines at ABIA airport. Southwest Airlines actually did quite well on on-time arrivals. With more than 15,000 flights arriving at Austin, Southwest Airlines only had averagely 5-min delay.

```{r,echo=F,message=FALSE, warning=FALSE}
# Select only those flights by the Top 3 airlines
top3 = airport[which(airport$UniqueCarrier %in% c('WN', 'AA', 'CO')),]
top3_to_austin = top3[top3['Dest']=="AUS",]
top3_from_austin = top3[top3['Dest']!="AUS",]
```

```{r,echo=F,message=FALSE, warning=FALSE}
# Arrival Delay of the Top 3 airlines by Month
p5 <- ggplot(data=top3_to_austin, aes(x=Month, y=ArrDelay))+ stat_summary(fun=mean, geom='bar')+ scale_x_continuous(breaks=c(1,2,3,4,5,6,7,8,9,10,11,12))
p5 + facet_wrap(~UniqueCarrier)+ ggtitle("Arrival Delay of the Top 3 airlines by month")+ aes(fill=UniqueCarrier)+
  scale_fill_brewer(palette="Dark2")
```
```{r,echo=F,message=FALSE, warning=FALSE}
# Departure Delay of the Top 3 airlines by Month
p5 <- ggplot(data=top3_from_austin, aes(x=Month, y=DepDelay))+ stat_summary(fun=mean, geom='bar')+ scale_x_continuous(breaks=c(1,2,3,4,5,6,7,8,9,10,11,12))
p5 + facet_wrap(~UniqueCarrier)+ ggtitle("Departure Delay of the Top 3 airlines by month")+ aes(fill=UniqueCarrier)+
  scale_fill_brewer(palette="Dark2")
```
By looking into monthly arrival and departure delay of the Top 3 airline, we came to realize that the season from September to November generally had low delay rate while March, June and December had high delay rate. 

```{r,echo=F,message=FALSE, warning=FALSE}
### Map
library(usmap)
code = read.csv(url('https://raw.githubusercontent.com/datasets/airport-codes/master/data/airport-codes.csv'))

# Split 'coordinates' into 'latitude' and 'longitude'
code$latitude <- as.numeric(sapply(strsplit(code$coordinates, ", "),"[", 1))
code$longitude <- as.numeric(sapply(strsplit(code$coordinates, ", "),"[", 2))

# Departure airport map - only US airports
airport.Dep <- merge(top3_to_austin, code, by.x = 'Origin', by.y = 'local_code')
airport.Dep.US <- filter(airport.Dep, iso_country == 'US')
#Project Map
airport.Dep_map <- airport.Dep.US %>%
  select(longitude, latitude, everything())
airport.Dep_T <- usmap_transform(airport.Dep_map)
#Plot
AA_dep_map <- plot_usmap() + geom_point(data = airport.Dep_T[airport.Dep_T['UniqueCarrier']=='AA',], aes(x=longitude.1, y=latitude.1),
                                        color='#1B9E77', size = 5)+ ggtitle("Departure airport of AA")
CO_dep_map <- plot_usmap() + geom_point(data = airport.Dep_T[airport.Dep_T['UniqueCarrier']=='CO',], aes(x=longitude.1, y=latitude.1),
                                        color='#D95F02', size = 5)+ ggtitle("Departure airport of CO")
WN_dep_map <- plot_usmap() + geom_point(data = airport.Dep_T[airport.Dep_T['UniqueCarrier']=='WN',], aes(x=longitude.1, y=latitude.1),
                                        color='#7570B3', size = 5)+ ggtitle("Departure airport of WN")
grid.arrange(AA_dep_map, CO_dep_map, WN_dep_map, ncol=3)
```

```{r,echo=F,message=FALSE, warning=FALSE}
#arrival delay by departure airport
ggplot(data=top3_to_austin[top3_to_austin['UniqueCarrier']=='AA',], aes(x=Origin, y=ArrDelay, group=1))+ stat_summary(fun=mean, geom='line', color='#1B9E77')+ ggtitle('Arrival delay by departure of AA')
ggplot(data=top3_to_austin[top3_to_austin['UniqueCarrier']=='CO',], aes(x=Origin, y=ArrDelay, group=1))+ stat_summary(fun=mean, geom='line', color='#D95F02')+ ggtitle('Arrival delay by departure of CO')
ggplot(data=top3_to_austin[top3_to_austin['UniqueCarrier']=='WN',], aes(x=Origin, y=ArrDelay, group=1))+ stat_summary(fun=mean, geom='line', color='#7570B3')+ ggtitle('Arrival delay by departure of WN')
```
From the above plots, we can know the these three airlines did not have same flight routes. \
For Southwest Airlines, flights from Nashville International Airport(BNA) generally had longest average of arrival delay. Surprisingly, flights from Las Vegas Airport(LAS) and Orlando International Airport(OIA) usually arrived earlier than the scheduled time. \
For American Airlines, most of the arrival delay are caused by flights that flied from Raleigh–Durham International Airport(RDU) in North Carolina, with average arrival delay at 30 minutes. \
For Continental Airlines, there were only two routes. The route from Newark Liberty International Airport(EWR) had more than twice delay time than that from George Bush Intercontinental Airport(IAH). As IAH is also located in Texas, the distance can be the factor of the delay.


# Portfolio modeling

We constructed three different portfolios of ETFs and used bootstrap resampling to estimate the 4-week (20 trading day) value at risk of each of three portfolios at the 5% level.

We selected 4 ETFs and invested respectively with different portion of money to be our 3 portfolios. The category of three portfolios is: Save, High Risk and Combined. We considered the date starting from 01-Jan-2015

* Save portfolios: VOO (45%), DIA (45%), TNA (5%), FAS (5%)
* High risk portfolio: TNA (45%), FAS (45%), VOO (5%), DIA (5%)
* Combined: VOO (25%), DIA (25%), TNA (25%), FAS (25%)

Brief introduction to our selection:
* DIA, VOO are three of the safest and largest ETF.
* FAS (Direxion Daily Financial Bull 3X shares): it delivers three time the losses when things lurch to the downside. As the pandemic wore down many local economics, this leveraged ETF took a massive hit and remains down about 50% year to date in 2020.
* TNA (Direxion Daily Small Cap Bull 3X Shares): with a smaller overall market that small companies are naturally more volatile.

#### Volatility of the ETFs across the 6-year period:
```{r,echo=F,message=FALSE, warning=FALSE}
#safety
rm(list = ls())
library(mosaic)
library(quantmod)
library(foreach)

myetf = c("DIA", "VOO", "TNA","FAS")
getSymbols(myetf)

DIA <- getSymbols('DIA', from='2015-01-01', to = '2021-08-06',auto.assign = 0)
VOO <- getSymbols('VOO', from='2015-01-01', to = '2021-08-06',auto.assign = 0)
TNA <- getSymbols('TNA', from='2015-01-01', to = '2021-08-06',auto.assign = 0)
FAS <- getSymbols('FAS', from='2015-01-01', to = '2021-08-06',auto.assign = 0)

DIAa = adjustOHLC(DIA)
VOOa = adjustOHLC(VOO)
TNAa = adjustOHLC(TNA)
FASa = adjustOHLC(FAS)

plot(ClCl(DIAa))
plot(ClCl(VOOa))
plot(ClCl(TNAa))
plot(ClCl(FASa))

myetf = c("DIA", "VOO", "TNA","FAS")
myprices = getSymbols(myetf, from = "2015-01-01")

for(ticker in myetf) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

# Combine all the returns in a matrix
all_returns = cbind( ClCl(DIAa),
                     ClCl(VOOa),
                     ClCl(TNAa),
                     ClCl(FASa))
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))

# Compute the returns from the closing prices
pairs(all_returns)

# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)
```

#### 1st Portfolio: Safe
Initial wealth: $100,000

We distributed our 90% of the total wealth to DIA and VOO, 10% to TNA and FAS.
```{r,echo=F,message=FALSE, warning=FALSE}
# Safe portfolio
# Update the value of  holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(0.45,0.45,0.05,0.05)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute new total wealth
holdings
total_wealth = sum(holdings)
total_wealth

## begin block
total_wealth = 100000
weights = c(0.45,0.45,0.05,0.05)
holdings = weights * total_wealth
n_days = 20  # capital T in the notes
return_of_investments = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
  return.today = resample(all_returns, 1, orig.ids=FALSE)  # sampling from R matrix in notes
  holdings = holdings + holdings*return.today
  total_wealth = sum(holdings)
  return_of_investments[today] = total_wealth
}
total_wealth
plot(return_of_investments, type='l', main='Safe Portfolio: Return over 20 days')

## end block

#simulate many different possible futures

initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.45,0.45,0.05,0.05)
  holdings = weights * total_wealth
  n_days = 20
  return_of_investments = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    return_of_investments[today] = total_wealth
  }
  return_of_investments
}

#plot all the 5000 simulations
trans_sim1 = rbind(rep(100000, 5000),t(sim1))
matplot(trans_sim1, type='l', col='grey', lty= 1)

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)
```

```{r,echo=F,message=FALSE, warning=FALSE}
# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)
```
```{r,echo=F,message=FALSE, warning=FALSE}
# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)
```
Summary - safe portfolio:
* Average return of investment after 20 days: 101791.8
* 5% value at Risk for safe portfolio: -8828.842
(results may be different every time running it)

#### 2nd Portfolio: High-risk
The initial wealth is $100,000

We distributed our 90% of the total wealth to TNA and FAS, 10% to DIA and VOO 

```{r,echo=F,message=FALSE, warning=FALSE}
# High risk portfolio
# Update the value of holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(0.05,0.05,0.45,0.45)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute new total wealth
holdings
total_wealth = sum(holdings)
total_wealth

## begin block
total_wealth = 100000
weights = c(0.05,0.05,0.45,0.45)
holdings = weights * total_wealth
n_days = 20  # capital T in the notes
return_of_investments = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
  return.today = resample(all_returns, 1, orig.ids=FALSE)  # sampling from R matrix in notes
  holdings = holdings + holdings*return.today
  total_wealth = sum(holdings)
  return_of_investments[today] = total_wealth
}
total_wealth
plot(return_of_investments, type='l', main='High Risk Portfolio: Return over 20 days')

## end block

#simulate many different possible futures

initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.05,0.05,0.45,0.45)
  holdings = weights * total_wealth
  n_days = 20
  return_of_investments = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    return_of_investments[today] = total_wealth
  }
  return_of_investments
}

#plot all the 5000 simulations
trans_sim1 = rbind(rep(100000, 5000),t(sim1))
matplot(trans_sim1, type='l', col='grey', lty= 1)

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)
```

```{r,echo=F,message=FALSE, warning=FALSE}
# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)
```

```{r,echo=F,message=FALSE, warning=FALSE}
# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)
```
Summary - high-risk portfolio:
* Average return of investment after 20 days: 105411.8
* 5% value at Risk for high-risk portfolio: -23499.82
(results may be different every time running it)

#### 3rd Portfolio: Combined
The initial wealth is $100,000

We distributed our 50% of the total wealth to TNA and FAS, 50% to DIA and VOO 

```{r,echo=F,message=FALSE, warning=FALSE}
# Combined Portfolio
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(0.25,0.25,0.25,0.25)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute new total wealth
holdings
total_wealth = sum(holdings)
total_wealth

## begin block
total_wealth = 100000
weights = c(0.25,0.25,0.25,0.25)
holdings = weights * total_wealth
n_days = 20  # capital T in the notes
return_of_investments = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
  return.today = resample(all_returns, 1, orig.ids=FALSE)  # sampling from R matrix in notes
  holdings = holdings + holdings*return.today
  total_wealth = sum(holdings)
  return_of_investments[today] = total_wealth
}
total_wealth
plot(return_of_investments, type='l', main='Safe Portfolio: Return over 20 days')

##end block

#simulate many different possible futures
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.25,0.25,0.25,0.25)
  holdings = weights * total_wealth
  n_days = 20
  return_of_investments = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    return_of_investments[today] = total_wealth
  }
  return_of_investments
}

#plot all the 5000 simulations
trans_sim1 = rbind(rep(100000, 5000),t(sim1))
matplot(trans_sim1, type='l', col='grey', lty= 1)

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)

```

```{r}
# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)
```

```{r}
# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)
```
Summary - combined portfolio:
* Average return of investment after 20 days: 103723.9
* 5% value at Risk for combined portfolio: -16008.71
(results may be different every time running it)

#### Conclusion
We can discover that the safe portfolio has the lowest 5% VaR, the high-risk portfolio has the highest 5% VaR, and the combined portfolio performed between the two portfolios. Moreover, we can obviously notice that the risk portfolio had more dramatic fluctuation.


# Market segmentation

```{r}
library(readr)
library(tibble)
social1 = read_csv(url("https://raw.githubusercontent.com/jgscott/STA380/master/data/social_marketing.csv"))
social1 = data.frame(social1)
social <- social1[-1]
row.names(social) <- social1$X1

social_results = social1 %>% group_by(X1) %>% summarize_all(mean) %>%
  column_to_rownames(var="X1")

  #variables correlation plot
cor(social_results)
ggcorrplot::ggcorrplot(cor(social_results),hc.order = T)
  #shopping,chatter,photo sharing
  #politics,travel,computers
  #art,tv_films,craft
  #parenting,religion,sport_fandom,food,school,family
  #automotive,news
  #personal_fitness,health_nutrition,outdoors,eco
  #fashion,beauty,cooking,music

Z = social/rowSums(social)
pc = prcomp(Z, scale=TRUE, rank=2)
plot(pc)
summary(pc)
round(pc$rotation[,1:2],2) 

loadings = pc$rotation
scores = pc$x
summary(pc)
plot(pc)

o1 = order(loadings[,1], decreasing=TRUE)
colnames(Z)[head(o1,10)]
colnames(Z)[tail(o1,10)]

o2 = order(loadings[,2], decreasing=TRUE)
colnames(Z)[head(o2,10)]
colnames(Z)[tail(o2,10)]
row.names(loadings[order(loadings[,1] ),][13:26,])

qplot(scores[,1], scores[,2],  xlab='Component 1', ylab='Component 2',alpha=0.1)

#save clustered customers to list for further target marketing use
list1 = row.names(scores[scores['PC1']<0 & scores['PC2']<0,])
list2 = row.names(scores[scores['PC1']<2 & scores['PC2']>0,])
list3 = row.names(scores[scores['PC1']>2 & scores['PC2']>-2,])

```

Based on the tweets by followers of NutrientH20 on social media, we have discovered customer segments that can be used for targeted social marketing in the future. With the available data where each user's tweets are labeled with interests, we are able to perform principle component analysis to discover the underlying clusters among them.

In the analysis, a correlation plot between each interests was made, and the correlations put together reasonable matches of variables, proving the legitimacy and value of the data.

The results from our principle component analysis creates three significant user segments as seen from the plot created. Below are the three segments with their major interests based on the two principle components.

* Segment 1 (lower left): personal_fitness, health_nutrition, online_gaming, college_uni, religion, eco, dating, food, fashion.

* Segment 2 (top left): sprts_playing, music, business, tv_film, small_business, religion, eco, dating, food, fashion.

* Segment 3 (top right): religion,sports_fandom,parenting,food,school,politics,travel,shopping,automotive, current events.

We suggested that these three segments were different in age and social status, where segment 1 is likely to consist of mostly students, segment 2 is consist of young adults, and segment 3 are people who have family or relatively older.
According to these traits and interests, NutrientH20 can perform targeted social media marketing by providin specific contents to each of the segments. We have saved the user id in list for each of the segments so that they can be specifically advertised via digital ads.

# Author attribution

```{r}
# Load in packages
library(tm)
library(tidyverse)
library(slam)
library(proxy)
library(rpart)
library(class)
library(randomForest)
```

First, let's create a function to read plain text and load in raw data.
The documents in the 'C50train' will be our training set while those in the 'C50test' will be our test set.
```{r, echo=T,message=FALSE, warning=FALSE}

# Reader plain text
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
# Load data
dirstrain <- list.dirs("C50train", full.names = T)
dirstest <- list.dirs("C50test", full.names = T)
# remove the first element in the list, making it to be 50 elements
dirstrain <- dirstrain[-1]
dirstest <- dirstest[-1]
```

#### TRAINING SET 

Let's deal with training set first. We rolled all 2500 directories from 50 authors in 'C50train'  together into a single corpus. Then we cleaned of punctuation, excessive white-space and common English language words. This pre-processing process facilitates the relevant terms to surface for text mining that would help build classification model.
```{r, echo=T,message=FALSE, warning=FALSE}
# Rolling directories together into a single corpus
file_list = Sys.glob(paste0(dirstrain,'/*.txt'))
# a more clever regex to get better file names
data = lapply(file_list, readerPlain) 
mynames = file_list %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist
names(data) = mynames
# Labeling with only names of authors
labelstrain <- gsub("C50train/(.*)/.*","\\1", file_list)

# Create the corpus
documents_raw = Corpus(VectorSource(data))

# Pre-processing
my_documents = documents_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation 
  tm_map(content_transformer(stripWhitespace)) %>%      # remove excess white-space
  tm_map(content_transformer(removeWords), stopwords("en")) # remove stopwords. 
```

We then created a document term matrix of the corpus. The raw results indicated that our training corpus had 2500 documents an 32570 terms. In our case, the sparsity index of 99% indicated that 99% of our DTM entries are zeros.
```{r, echo=T,message=FALSE, warning=FALSE}
# Create a doc-term-matrix from the corpus
DTMtrain = DocumentTermMatrix(my_documents)
# DTM's summary statistics
DTMtrain  # XX% sparsity means XX% of the entries are zero
```

We could see that the noise of the "long tail"(rare terms) was actually huge. We could not learn much on those terms occurred once. As a result, we removed those terms that have count 0 in 95% of documents. The new results showed that now we only had 801 terms in the corpus and the sparsity is 86%.
```{r, echo=T,message=FALSE, warning=FALSE}
# Removes those terms that have count 0 in >95% of docs.  
DTMtrain = removeSparseTerms(DTMtrain, 0.95)
DTMtrain
```
Let's try to inspect the terms that appear in at least 250 documents:
```{r, echo=T,message=FALSE, warning=FALSE}
findFreqTerms(DTMtrain, 250)
```

#### TEST SET

We did the same pre-processing process and create a document term matrix for our test corpus.
```{r, echo=T,message=FALSE, warning=FALSE}
file_list_test = Sys.glob(paste0(dirstest,'/*.txt'))
data_test = lapply(file_list_test, readerPlain) 
mynames_test = file_list_test %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist
names(data_test) = mynames_test

labelstest <- gsub("C50test/(.*)/.*","\\1", file_list_test)

# Create the corpus
documents_test = Corpus(VectorSource(data_test))

# Pre-processing
my_documents_test = documents_test %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation 
  tm_map(content_transformer(stripWhitespace)) %>%      # remove excess white-space
  tm_map(content_transformer(removeWords), stopwords("en")) # remove stopwords.
```

We could find out that our test corpus had 2500 documents an 33373 terms. The sparsity is 99%.
```{r, echo=T,message=FALSE, warning=FALSE}
## create a doc-term-matrix from the corpus
DTMtest = DocumentTermMatrix(my_documents_test)
DTMtest
```
After removing those therms that have count 0 in 95% of the documents, we got 816 terms and sparsity at 86% for our test corpus.
```{r, echo=T,message=FALSE, warning=FALSE}
DTMtest = removeSparseTerms(DTMtest, 0.95)
DTMtest
```

There was new words in the test data that we never saw in the training set. We decided to ignore these new terms in the test data and aligned the terms in the training data with those in the test data. Now we could see that we had 743 common words in training and test.
```{r, echo=T,message=FALSE, warning=FALSE}
# Covert from matrix to DataFrame
traindata <- data.frame( as.matrix( DTMtrain ), label = labelstrain)
traindata$label <- factor(traindata$label)
testdata <- data.frame( as.matrix( DTMtest ), label = labelstest)
testdata $label <- factor(testdata $label)

# Aligning Training data terms with Test data terms
traindata2 <- traindata[, names(traindata) %in% names(testdata) ]
testdata2 <- testdata[, names(traindata2) ]
```

### MODEL: DECISION TREE
```{r, echo=T,message=FALSE, warning=FALSE}
# Build a decision tree model
model_tree <- rpart(label ~ .,data = traindata2)

# Make predictions on testing data
preds <- predict(model_tree,  testdata2, type = "class")

# Calculate accuracy
accuracy_tree <- mean(testdata2$label == preds)
accuracy_tree
```
Our decision tree models achieved 23.96% accuracy.

### MODEL: KNN
```{r, echo=T,message=FALSE, warning=FALSE}
set.seed(2021)
accuracy_knn <- c()

# Make predictions with different k values
for(k in c(1, 3, 5, 7, 9, 15, 30, 50, 70)) {
preds <- knn(traindata2[,-ncol(traindata2)], 
             testdata2[,-ncol(testdata2)],
             traindata2$label,
             k = k)
accuracy_knn <- c(accuracy_knn,  mean(testdata2$label == preds))
}
  
cat("accuracy for different k values:", accuracy_knn)
cat("\nThe best accuracy = ", max(accuracy_knn))
bestk <- c(1, 3, 5, 7, 9, 15, 30, 50, 70)[which.max(accuracy_knn)]
cat("\nThe k value with best accuracy:", bestk)
```
From our knn analysis, we achieved best accuracy at 35.48% when k=1.

### MODEL: Random Forest
```{r, echo=T,message=FALSE, warning=FALSE}
#build a random forest model
set.seed(2021)
model_rf <- randomForest(label ~ .,data = traindata2)

#make predictions on testing data
preds <- predict(model_rf,  testdata2, type = "class")

accuracy_rf <- mean(testdata2$label == preds)
accuracy_rf
```
Our random forest model helped us achieve 60.68% accuracy. As a result, we can conclude that the random forest model is best here at predicting the author of an article on the basis of that article's textual content.


# Association rule mining
We first transformed the data frame into transaction type, and we utilized summary to discover information about the data. We noticed that there were 9835 transactions, 169 items and a density of 0.02609146. Moreover, we got the list of most frequent items, and whole milk is the most common item in transactions. There also showed numbers of item in each transaction.

```{r, echo=T,message=FALSE, warning=FALSE}
rm(list = ls())
library(tidyverse)
library(arules) 
library(arulesViz)

groceries <- read.transactions("https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt", format="basket", sep=",")

summary(groceries)
dim(groceries)  
basketSize<-size(groceries)  

summary(basketSize)
sum(basketSize) 
itemFreq <- itemFrequency(groceries)
itemFreq[1:5]  

#average item in each transaction
sum(itemFreq)

itemFrequencyPlot(groceries, topN=10) 

image(groceries[1:10])  
```

First, we set support = 0.05, confidence = 0.1 and at least 2 items to conduct Apriori algorithm:
```{r, echo=T,message=FALSE, warning=FALSE}
groceryrules <- apriori(groceries, parameter = list(support = 0.05, confidence = 0.1, minlen = 2))  
summary(groceryrules)  

inspect(groceryrules[1:6])  
par(family = 'STKaiti')
groceryrules %>% plot() 
par(family = 'STKaiti')
groceryrules %>% head(10) %>% 
  plot(., method = "graph")
```
Since we set high support and low confidence, there were only 6 rules. We can realize that whole mike, rolls buns and yogurt have strong relationship in those transactions. The result also showed the same situation as item frequency plot.

Let’s increase confidence and support and maintain at least 2 items:
```{r, echo=F,message=FALSE, warning=FALSE}
#decrease confidence and increase support 
groceryrules2 <- apriori(groceries, parameter = list(support = 0.03, confidence = 0.3, minlen = 2))  
summary(groceryrules2)  


inspect(groceryrules2[1:14])

par(family = 'STKaiti')
groceryrules2 %>% plot() 
par(family = 'STKaiti')
groceryrules2 %>% head(10) %>% 
  plot(., method = "graph")
```
We got 14 rules from the second output. There were more items appeared in transactions. Still, the whole milk played as the most common role as appearance.

Let’s try another combination by continuing increasing confidence, decreasing support and maintaining at least 2 items: 
```{r}
#groceryrules3, keep decreasing confidence and increasing support
groceryrules3 <- apriori(groceries, parameter = list(support = 0.01, confidence = 0.5, minlen = 2))  
summary(groceryrules3)  


inspect(groceryrules3[1:10])
par(family = 'STKaiti')
groceryrules3 %>% plot()
par(family = 'STKaiti')
groceryrules3 %>% head(10) %>% 
  plot(., method = "graph")
```
The third result gave us 15 rules with more items appeared while whole milk was still the most common one.

Summary:
* Whole milk is the most common item in transactions.
* Consumers are more likely to buy other vegetables when buying root vegetables.
